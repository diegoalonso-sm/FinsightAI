import asyncio

from crawl4ai import AsyncWebCrawler
from crawl4ai import BrowserConfig, CacheMode
from crawl4ai.async_configs import CrawlerRunConfig
# from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy
from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer
import time

from crawl4ai.deep_crawling.filters import (
    FilterChain,
    URLPatternFilter,
    DomainFilter,
)

# TODO: Considerar utilizar estrategias de streaming para paralelizar el scraping.
# TODO: Revisar por qu√© no se est√°n imprimiendo correctamente los scores.
# TODO: Probar filtros del tipo SEOFilter y ContentRelevanceFilter.
# TODO: En caso de fallar el crawler para un URL, lanzar una excepci√≥n.


async def simple_scroll_time(url, duration_seconds=15):

    session_id = "scroll_session"

    browser_config = BrowserConfig(
        headless=False,
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:

        config_initial = CrawlerRunConfig(
            wait_for="css:body",
            session_id=session_id,
            cache_mode=CacheMode.BYPASS,
        )

        result = await crawler.arun(url=url, config=config_initial)

        print(f"üåê P√°gina cargada. Scrolleando durante {duration_seconds} segundos...")

        start_time = time.time()

        scroll_count = 0

        while (time.time() - start_time) < duration_seconds:
            scroll_js = "window.scrollTo(0, document.body.scrollHeight);"

            config_scroll = CrawlerRunConfig(
                session_id=session_id,
                js_code=scroll_js,
                js_only=True,
                cache_mode=CacheMode.BYPASS,
            )

            await crawler.arun(url=url, config=config_scroll)
            scroll_count += 1
            print(f"üñ±Ô∏è Scroll n√∫mero {scroll_count} hecho.")

            await asyncio.sleep(1)  # Esperar 1 segundo entre scrolls (puedes ajustar si quieres m√°s r√°pido o lento)

        print(f"üéØ Scrolling finalizado tras {scroll_count} scrolls en {duration_seconds} segundos.")

        print(result.links)



async def yahoo_finance_crawler():

    browser_config = BrowserConfig(
        headless=False,
        text_mode=True,
        light_mode=True,
    )

    filter_chain = FilterChain([
        URLPatternFilter(patterns=["*/news/*"]),
        DomainFilter(
            allowed_domains=[
                "finance.yahoo.com"
            ],
        ),
    ])

    crawling_strategy = BestFirstCrawlingStrategy(
        max_depth=1,
        include_external=False,
        # max_pages=100,
        filter_chain=filter_chain,
    )

    run_config = CrawlerRunConfig(
        word_count_threshold=200,
        deep_crawl_strategy=crawling_strategy,
        cache_mode=CacheMode.BYPASS,
        excluded_tags=['form', 'scripts', 'style'],
        keep_data_attributes=False,
        stream=True,
        #exclude_external_links=True,
        #remove_overlay_elements=True,
        #pdf=True,
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:

        results = []

        async for result in await crawler.arun(
            url="https://finance.yahoo.com/news",
            config = run_config,
        ):

            results.append(result)
            score = result.metadata.get("score", 0)
            print(f"Score: {score:.2f} | {result.url}")

            if not result.success:
                print(f"Crawl failed: {result.error_message}")
                print(f"Status code: {result.status_code}")

            if result.pdf:
                with open("wikipedia_page.pdf", "wb") as f:
                    f.write(result.pdf)

        # print(result.cleaned_html)

        print(f"Crawled {len(results)} high-value pages")


if __name__ == "__main__":

    # asyncio.run(yahoo_finance_crawler())
    asyncio.run(simple_scroll_time("https://finance.yahoo.com/news", duration_seconds=5))